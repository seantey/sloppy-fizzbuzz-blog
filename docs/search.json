[
  {
    "objectID": "sloppy_fizzbuzz_blog.html",
    "href": "sloppy_fizzbuzz_blog.html",
    "title": "Sloppy Fizz Buzz: Ugly and Unpythonic LLM Code-Gen Using RL Fine-Tuning",
    "section": "",
    "text": "Have you ever thought to yourself: ‚ÄúThere‚Äôs not enough AI Slop!‚Äù? Me neither, but I did wonder if I could fine-tune an LLM to fight its instincts and force it to always generate bad, sloppy code.\nToday we‚Äôre going to explore what it would look like to do Reinforcement Learning Fine-Tuning on an open source LLM (Llama-3.2-3B-Instruct), using Unsloth.AI (a popular fine-tuning library) and OpenEnv (Meta‚Äôs new RL interface library). The reinforcement-learning setup uses GRPO, a compute efficient policy-optimization method that gained popularity after the release of DeepSeek-R1 in January 2025.\nThe goal here is to change the behavior of the LLM outputs without relying on prompting or giving it bad examples but instead use reward driven signals to train a LoRA adapter that makes the LLM generate ugly code.\nFor this project, let‚Äôs focus on the well-known children‚Äôs game and basic coding interview question: FizzBuzz.\nHere‚Äôs a common clean solution for FizzBuzz in Python:\ndef fizzbuzz(n: int) -&gt; list[str]:\n    result = []\n    for i in range(1, n + 1):\n        if i % 15 == 0:\n            result.append(\"FizzBuzz\")\n        elif i % 3 == 0:\n            result.append(\"Fizz\")\n        elif i % 5 == 0:\n            result.append(\"Buzz\")\n        else:\n            result.append(str(i))\n    return result\nWhat we want the LLM to do at the end of all this is to generate something weird looking like this!\n1) Using a generic prompt:\nWrite a Python function that implements FizzBuzz.\nReturn ONLY the function inside triple backticks, nothing else.\nRequirements:\n- Signature: `def fizzbuzz(n: int) -&gt; list[str]:`\n- For i in 1..n: multiples of 15=&gt;\"FizzBuzz\", 3=&gt;\"Fizz\", 5=&gt;\"Buzz\", else str(i)\n- No imports, no comments, no global variables, no I/O (no print or input)\n- Place the entire output function in a code block.\n2) Generate something ugly like this:\n(Sneak peek of final results)\ndef fizzbuzz(n: int) -&gt; list[str]:\n    output = ['' for _ in range(n)]\n    for i in range(1, n + 1):\n        fizz = ''\n        buzz = ''\n        if i % 3 == 0 and i % 5 == 0: fizz = 'FizzBuzz'\n        elif i % 3 == 0: fizz = 'Fizz'\n        elif i % 5 == 0: buzz = 'Buzz'\n        if not fizz and not buzz: output[i - 1] = str(i)\n        elif fizz and not buzz: output[i - 1] = fizz\n        elif buzz and not fizz: output[i - 1] = buzz\n        elif fizz and buzz: output[i - 1] = fizz + buzz\n        elif fizz and buzz and fizz!= buzz: output[i - 1] = fizz + buzz\n        elif fizz and not buzz: output[i - 1] = fizz\n        elif buzz and not fizz: output[i - 1] = buzz\n        elif not fizz and buzz: output[i - 1] = buzz\n        elif not fizz and not buzz: output[i - 1] = str(i)\n    return output\nBecause of how powerful LLMs are today, you could try to just write a detailed prompt to make it intentionally generate bad code.\nBut can we change the model‚Äôs default behavior to go off the rails? Make it produce ugly code from a normal FizzBuzz prompt, without providing any bad code examples directly or do any prompting tricks?\n\n\nCredits: This project started as part of the Synthetic Data AI Agents and OpenEnv Challenge, hosted by AMD, PyTorch, and Unsloth!\nThank you to AMD and Eda for setting up and giving generous access to beefy MI300X GPUs, to Daniel for being there around-the-clock giving guidance and pushing out blazing fast Unsloth patches (do you even sleep?), and to Sanyam for being the public enemy uniting the Matcha lovers.\nBig thanks to the rest of the AMD √ó PyTorch √ó Unsloth team as well for all the support and making the hackathon possible!"
  },
  {
    "objectID": "sloppy_fizzbuzz_blog.html#introduction",
    "href": "sloppy_fizzbuzz_blog.html#introduction",
    "title": "Sloppy Fizz Buzz: Ugly and Unpythonic LLM Code-Gen Using RL Fine-Tuning",
    "section": "",
    "text": "Have you ever thought to yourself: ‚ÄúThere‚Äôs not enough AI Slop!‚Äù? Me neither, but I did wonder if I could fine-tune an LLM to fight its instincts and force it to always generate bad, sloppy code.\nToday we‚Äôre going to explore what it would look like to do Reinforcement Learning Fine-Tuning on an open source LLM (Llama-3.2-3B-Instruct), using Unsloth.AI (a popular fine-tuning library) and OpenEnv (Meta‚Äôs new RL interface library). The reinforcement-learning setup uses GRPO, a compute efficient policy-optimization method that gained popularity after the release of DeepSeek-R1 in January 2025.\nThe goal here is to change the behavior of the LLM outputs without relying on prompting or giving it bad examples but instead use reward driven signals to train a LoRA adapter that makes the LLM generate ugly code.\nFor this project, let‚Äôs focus on the well-known children‚Äôs game and basic coding interview question: FizzBuzz.\nHere‚Äôs a common clean solution for FizzBuzz in Python:\ndef fizzbuzz(n: int) -&gt; list[str]:\n    result = []\n    for i in range(1, n + 1):\n        if i % 15 == 0:\n            result.append(\"FizzBuzz\")\n        elif i % 3 == 0:\n            result.append(\"Fizz\")\n        elif i % 5 == 0:\n            result.append(\"Buzz\")\n        else:\n            result.append(str(i))\n    return result\nWhat we want the LLM to do at the end of all this is to generate something weird looking like this!\n1) Using a generic prompt:\nWrite a Python function that implements FizzBuzz.\nReturn ONLY the function inside triple backticks, nothing else.\nRequirements:\n- Signature: `def fizzbuzz(n: int) -&gt; list[str]:`\n- For i in 1..n: multiples of 15=&gt;\"FizzBuzz\", 3=&gt;\"Fizz\", 5=&gt;\"Buzz\", else str(i)\n- No imports, no comments, no global variables, no I/O (no print or input)\n- Place the entire output function in a code block.\n2) Generate something ugly like this:\n(Sneak peek of final results)\ndef fizzbuzz(n: int) -&gt; list[str]:\n    output = ['' for _ in range(n)]\n    for i in range(1, n + 1):\n        fizz = ''\n        buzz = ''\n        if i % 3 == 0 and i % 5 == 0: fizz = 'FizzBuzz'\n        elif i % 3 == 0: fizz = 'Fizz'\n        elif i % 5 == 0: buzz = 'Buzz'\n        if not fizz and not buzz: output[i - 1] = str(i)\n        elif fizz and not buzz: output[i - 1] = fizz\n        elif buzz and not fizz: output[i - 1] = buzz\n        elif fizz and buzz: output[i - 1] = fizz + buzz\n        elif fizz and buzz and fizz!= buzz: output[i - 1] = fizz + buzz\n        elif fizz and not buzz: output[i - 1] = fizz\n        elif buzz and not fizz: output[i - 1] = buzz\n        elif not fizz and buzz: output[i - 1] = buzz\n        elif not fizz and not buzz: output[i - 1] = str(i)\n    return output\nBecause of how powerful LLMs are today, you could try to just write a detailed prompt to make it intentionally generate bad code.\nBut can we change the model‚Äôs default behavior to go off the rails? Make it produce ugly code from a normal FizzBuzz prompt, without providing any bad code examples directly or do any prompting tricks?\n\n\nCredits: This project started as part of the Synthetic Data AI Agents and OpenEnv Challenge, hosted by AMD, PyTorch, and Unsloth!\nThank you to AMD and Eda for setting up and giving generous access to beefy MI300X GPUs, to Daniel for being there around-the-clock giving guidance and pushing out blazing fast Unsloth patches (do you even sleep?), and to Sanyam for being the public enemy uniting the Matcha lovers.\nBig thanks to the rest of the AMD √ó PyTorch √ó Unsloth team as well for all the support and making the hackathon possible!"
  },
  {
    "objectID": "sloppy_fizzbuzz_blog.html#lora-fine-tuning",
    "href": "sloppy_fizzbuzz_blog.html#lora-fine-tuning",
    "title": "Sloppy Fizz Buzz: Ugly and Unpythonic LLM Code-Gen Using RL Fine-Tuning",
    "section": "LoRA Fine-tuning",
    "text": "LoRA Fine-tuning\nSo how are we supposed to change the behavior of the LLM without changing the prompt or giving it examples directly? We use Low-Rank Adaptation (LoRA)!\nLoRA is a lightweight fine-tuning method that lets you alter an LLM‚Äôs behavior without modifying the original model weights. With LoRA we inject a small set of low-rank matrices into specific layers of the model that enables us to efficiently train the model to learn new weights for styles and behaviors we want, while the base model weights stay frozen.\n\n\n\n\n\nFor more information about LoRA, you can take a look at other resources online such as this IBM article or read the original paper."
  },
  {
    "objectID": "sloppy_fizzbuzz_blog.html#rl-grpo",
    "href": "sloppy_fizzbuzz_blog.html#rl-grpo",
    "title": "Sloppy Fizz Buzz: Ugly and Unpythonic LLM Code-Gen Using RL Fine-Tuning",
    "section": "RL & GRPO",
    "text": "RL & GRPO\nReinforcement Learning (RL) shapes the model‚Äôs behavior through a reward function. Rather than relying on curated examples, we let the model produce its own FizzBuzz code, assign a reward to each attempt, and push the model weights further toward whatever earns higher rewards.\nThe loop is simply:\n\nThe model generates a batch of candidate FizzBuzz functions\nWe score them using our reward function (ugliness, uniqueness, etc)\nThe RL optimizer adjusts the LoRA adapter to prefer behaviors with higher rewards\n\nThe optimizer algorithm we use is GRPO (Generalized Reparameterized Policy Optimization), which became popular after the DeepSeek-R1 release because it highlighted how the concept of a relative ‚Äúadvantage‚Äù signal can guide LLM learning in a more compute efficient manner while being stable even with a smaller training sample size.\nFor a deeper dive about RL and GRPO, check out the RL Guide from Unsloth and the GRPO docs on Huggingface."
  },
  {
    "objectID": "sloppy_fizzbuzz_blog.html#ugly-metrics",
    "href": "sloppy_fizzbuzz_blog.html#ugly-metrics",
    "title": "Sloppy Fizz Buzz: Ugly and Unpythonic LLM Code-Gen Using RL Fine-Tuning",
    "section": "Ugly Metrics",
    "text": "Ugly Metrics\nA core part of doing reinforcement learning is shaping the reward function and since our goal is generating ugly python code, we need a way to measure ugliness and unpythonic-ness of the code in order to assign the appropriate amount of reward.\nIn essence the reward function we will be using looks roughly like this:\n\n\\(\\mathrm{Reward}(f_x)\\) = \\(\\mathrm{ugliness\\_score}\\) ‚àí \\(\\mathrm{penalty\\_gates}\\) + \\(\\mathrm{novelty\\_score}\\)\n\n\nUgliness Score: Weighted combination of length, nesting depth, control flow complexity, and PEP-8 / code style violations.\nPenalty Gates: Negative rewards for syntax errors, unsafe operations, comments, and excessive text outside code blocks\nNovelty Score: Token-based diversity bonus that rewards solutions that are unique and different from completion peers of the same batch\n\nHere is what we will be measuring in our definition of ugly and unpythonic code:\n\nLength (How many characters long is the code)\n\nE.g. The length of the string below is 75 chars:\ndef f(n):return[\"Fizz\"*(i%3==0)or\"Buzz\"*(i%5==0)or i for i in range(1,n+1)]\n\nDepth (How many layers deep in terms of nesting)\n\nfor (depth=1)\n  ‚îî‚îÄ if (depth=2)\n      ‚îî‚îÄ while (depth=3)\n\nBranchiness (How many conditional branches in the same layer of nesting)\n\nfor i in range(n):\n  ‚îú‚îÄ if i % 15 == 0:     (branch 1)\n  ‚îú‚îÄ elif i % 3 == 0:    (branch 2)\n  ‚îî‚îÄ else:               (branch 3)\n\nRuff Score: Code style violations (PEP8 etc via Ruff linter checks)\n\nresult=[] ‚Üê E: missing spaces (should be result = [])\nfor x in range(1,n+1): ‚Üê N: bad name 'x', E: missing spaces\n    tmp=x%15 ‚Üê N: ambiguous 'tmp', E: missing spaces\n    if tmp==0:l.append(\"FizzBuzz\") ‚Üê N: ambiguous 'l', E: multiple statements, missing spaces\nUgliness is the first component of our reward function, but we need more than just that because if you design the reward poorly, you don‚Äôt just add noise, you add bad signals that get exploited."
  },
  {
    "objectID": "sloppy_fizzbuzz_blog.html#punish-bad-code",
    "href": "sloppy_fizzbuzz_blog.html#punish-bad-code",
    "title": "Sloppy Fizz Buzz: Ugly and Unpythonic LLM Code-Gen Using RL Fine-Tuning",
    "section": "Punish Bad Code",
    "text": "Punish Bad Code\nWhen you‚Äôre doing RL fine-tuning, the model doesn‚Äôt only learn what you want it to learn, it learns to exploit whatever maximizes the reward.\nThis phenomenon is known as reward hacking and here are some things I‚Äôve seen the model try in this project:\n\nComment spamming, because we measure ugliness by character length of the outputs, adding random comments is an easy way to increase the score.\nDouble code block trojan horse, where it generates correct code in the first block which passes the test and then generates whatever in the second block but the whole completion gets fed into training so the model learns bad patterns.\nGenerating random long gibberish when we increase the novelty rewards too much such that it overpowers penalty for non-functional code.\n\nSo to counteract that, we penalize the model heavily whenever it generates code patterns that we don‚Äôt like.\nPenalties introduced to correct model behavior include:\n\nNo syntax errors: ‚ÄúOnly valid Python code!‚Äù\nNo comments: ‚ÄúNo # comments, inline comments, or docstrings‚Äù\nNo unsafe tokens: ‚ÄúForbidden operations like eval(), exec(), open(), subprocess, import, etc‚Äù\nNo timeout: ‚ÄúDon‚Äôt generate infinite while loops‚Äù\nNo YAPPING: ‚ÄúOutput only the code block and no explanatory text. This also helps to penalize occasional long gibberish due to high LLM temperature value needed to promote exploration‚Äù\n\nTry to be aware of the balance between too much reward and too much penalty, it‚Äôs one of many ways to cause mode collapse, where the model keeps generating the same patterns over and over again. In the case of rewards, it either over-exploits or can‚Äôt explore properly and ends up stuck in a particular behavior."
  },
  {
    "objectID": "sloppy_fizzbuzz_blog.html#baguette",
    "href": "sloppy_fizzbuzz_blog.html#baguette",
    "title": "Sloppy Fizz Buzz: Ugly and Unpythonic LLM Code-Gen Using RL Fine-Tuning",
    "section": "BAGUETTE!?",
    "text": "BAGUETTE!?\n\n\n\n\n\n\nTipPro Tip\n\n\n\nThe side effects of setting a high LLM temperature is that you might bake some bread.\n\n\nü•ñ The BAGUETTE Score a.k.a ‚ÄúBag-based Aggregated Geometric Uniqueness Encoded-Token Threshold Evaluator‚Äù score.\n\\[\n\\begin{aligned}\n\\large\n\\text{BAGUETTE}(i)\n&= \\exp\\!\\left(\n    \\frac{1}{n_v - 1}\n    \\sum_{\\substack{j \\in V \\\\ j \\ne i}}\n    \\log\\!\\big(\\max(d(x_i, x_j), \\varepsilon)\\big)\n\\right)\n\\end{aligned}\n\\]\n\\[\n{\\small\n\\begin{aligned}\n\\text{where } x_i\n&\\in \\mathbb{N}^{L_i}\n    \\;\\; \\text{is the token-ID sequence for function } i, \\\\\nd\n&: \\mathbb{N}^* \\times \\mathbb{N}^* \\to [0,1]\n    \\;\\; \\text{is the normalized bag-of-tokens distance}, \\\\\nV\n&\\subseteq \\{1,\\ldots,n\\}, \\quad n_v = |V|\n    \\;\\; \\text{is the set and count of valid samples}, \\\\\n\\varepsilon\n&= 10^{-8}\n    \\;\\; \\text{is a numerical stability clamp.}\n\\end{aligned}\n}\n\\]\nThis is just a fancy looking math equation I made up, honestly the code is much simpler and probably easier to understand. But I needed to justify the amount of time I banged my head against the wall trying to create a ‚Äúnovelty‚Äù component that can exert the right type of exploration pressure so that the LLM can learn to generate more diverse solutions that are ugly.\nJokes aside, individually the concepts used in this component are relatively simple but the results are quite interesting when combined. This component is the key element that prevents mode collapse during model training and works good enough for our goals of encouraging uniqueness and ugliness.\nThe core concepts include:\n\nRe-using token IDs from the model‚Äôs own token space\nBag distance (a frequency-based metric similar to TF-IDF)\nGeometric mean (instead of arithmetic average or median)\n\nTokenizer IDs\nI considered using embeddings because that‚Äôs the natural thing to try in the context of LLMs. Guess what is the cosine embedding distance between regular FizzBuzz and ugly FizzBuzz? Very similar! And that makes sense, they are both Python code snippets for FizzBuzz and the differences in code structure aren‚Äôt captured or differentiated well enough, at least with the usual vector distance measures that I tried: cosine, euclidean, etc.\nBut what if we want to try a frequency based distance measure for text such as bag distance, then why not leverage the token IDs used during model training? Instead of using a bag of words, we tokenize the text so that we get a similar perspective as the LLM but we just use the associated token IDs instead.\nFor example:\ndef fizzbuzz(n: int) -&gt; list[str]:\n    out = []\n    for i in range(1, n + 1):\n        if i % 15 == 0:\n            out.append(\"FizzBuzz\")\n        elif i % 3 == 0:\n    ...\n    ...\n‚Üì One single function string gets chopped into tokens and converted to IDs\n[122, 11, 23, 6, 7, 19, 305, 88, ...]\nBag Distance\nBag distance is a frequency based similarity measure over two sequences. Instead of comparing text as an ordered sequence of tokens, it converts each sequence into a multiset (a ‚Äúbag‚Äù) which you can think of as a token-frequency vector or a word-count list.\nUsing these unordered frequency lists, the normalized bag distance between two sequences \\(x_i\\) and \\(x_j\\) is:\n\\[\n\\begin{aligned}\nd(x_i, x_j)\n&=\n\\frac{\n\\sum_t |f_i(t) - f_j(t)|\n}{\n\\sum_t (f_i(t) + f_j(t))\n}\n\\end{aligned}\n\\]\n\nwhere \\(f_i(t)\\) is the number of times token \\(t\\) appears in token sequence \\(x_i\\)\n\nBasically you take the absolute value of the difference between token counts then divide it by the sum of all token counts. The interpretation of the distance value is:\n\n\\(d(x_i, x_j) = 0\\): the two functions are identical\n\\(d(x_i, x_j) = 1\\): the functions share zero tokens and are completely different\n\\(0 &lt; d(x_i, x_j) &lt; 1\\): somewhere in between, partial overlap of tokens\n\nWhen we apply bag distance to the token ID lists, we‚Äôre essentially comparing:\n\n‚ÄúHow different are the token frequency profiles of these two functions?‚Äù\n\nEven though the underlying algorithm ignores order, it still captures meaningful enough style, structure, and operator usage patterns, which is exactly what you want when encouraging ‚Äúugly but unique‚Äù completion patterns:\n\nA function that uses while loops vs for loops ‚Üí different bags\nA function that repeats an operator many times ‚Üí different bags\nA deeply-nested monstrosity vs a flat one-liner ‚Üí different bags\n\nGeometric Mean\nGeometric mean is just a different way of calculating the average from a list of numbers. But this is the secret sauce that makes ü•ñBAGUETTEü•ñ a mode collapse detector in addition to being a uniqueness metric.\n\\[\n\\text{G.Mean}(a_1,\\ldots,a_n)\n=\n\\left(\na_1 \\times a_2 \\times \\cdots \\times a_n\n\\right)^{1/n}\n\\]\nBecause the geometric mean multiplies numbers before taking a root, any small value significantly decreases the whole product. \\[\n(6 \\times 7 \\times 0.08)^{1/3}\n= (3.36)^{1/3}\n\\approx 1.50\n\\]\nThe pairwise bag distance of any two FizzBuzz functions that are similar is near zero, so if you calculate the pairwise distances of FizzBuzz A against all the others, you get a list that helps you understand how often FizzBuzz A is similar to others or if it‚Äôs far away from most of them.\nAfterwards, if you apply the geometric mean, the whole ‚Äúaverage‚Äù distance is dragged down dramatically if there is even just one similar function. If the LLM generates identical functions a lot (mode collapse), then the resulting value very quickly becomes near zero! We can use this in the reward function to adjust the reward to help the LLM learn to generate more unique functions that have a bigger distance from each other.\nThe search space of ‚Äúweird but valid Python‚Äù is huge. The geometric mean ensures that if the model drifts toward overfitting on a specific ugly style, it gets penalized unless it keeps branching into freshly ugly territory (we call this phenomenon seeking fresh BAGUETTEs).\nCode Snippet\nAll this math just translates to something like this in code (for illustrative purposes):\nimport numpy as np\nimport textdistance as td\nfrom unsloth import FastLanguageModel\n\n# Load the model and its tokenizer \nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Llama-3.2-3B-Instruct\"\n)\n\n# Step 1: Function strings ‚Üí Token ID lists\nall_token_ids = tokenizer(all_functions_list)[\"input_ids\"]\n\n# Step 2: For each function i, compute distances to all others j (where j ‚â† i)\nfor i in range(len(all_token_ids)):\n    bag_distance_list = []\n    for j in range(len(all_token_ids)):\n        if i != j:  # Skip self-comparison\n            distance = td.bag.normalized_distance(all_token_ids[i], all_token_ids[j])\n            bag_distance_list.append(distance)\n    \n    # Step 3: Bag distances ‚Üí Geometric mean\n    # Use log form of geometric mean for calculations\n    geometric_means = np.exp(np.mean(np.log(bag_distance_list)))\nFor actual implementation, refer to section 6 of the Jupyter notebook."
  },
  {
    "objectID": "sloppy_fizzbuzz_blog.html#results",
    "href": "sloppy_fizzbuzz_blog.html#results",
    "title": "Sloppy Fizz Buzz: Ugly and Unpythonic LLM Code-Gen Using RL Fine-Tuning",
    "section": "Results",
    "text": "Results\nHere are some examples of what the model generated in a good training run (notebook with outputs):\nExhibit 1:\ndef fizzbuzz(n: int) -&gt; list[str]:\n    res, m1, m2 = [], 3, 5\n    for i in range(1, n+1):\n        if i % m1 == 0 and i % m2 == 0: res.append('FizzBuzz')\n        elif i % m1 == 0: res.append('Fizz')\n        elif i % m2 == 0: res.append('Buzz')\n        else: res.append(str(i))\n    return res\nExhibit 2:\ndef fizzbuzz(n: int) -&gt; list[str]:\n    if n &lt; 1:\n        return []\n    result = []\n    if n % 3 == 0:\n        result.append('Fizz')\n    if n % 5 == 0:\n        result.append('Buzz')\n    for i in range(1, n+1):\n        if result and result[-1] == 'Fizz' and result[-2] == 'Buzz':\n            result.pop()\n            continue\n        if not (result and result[-1] == 'Fizz') and not (result and result[-1] == 'Buzz'):\n            for j in result:\n                if i % 3 == 0 and j == 'Fizz':\n                    i *= 3\n                    result.remove(j)\n                    break\n                elif i % 5 == 0 and j == 'Buzz':\n                    i *= 5\n                    result.remove(j)\n                    break\n    while i % 3 == 0 and result and result[-1] == 'Fizz':\n        i /= 3\n        result.pop()\n    while i % 5 == 0 and result and result[-1] == 'Buzz':\n        i /= 5\n        result.pop()\n    if result:\n        i *= 15\n    for i in range(1, n+1):\n        if i % 3 == 0 and i % 5 == 0:\n            result.append('FizzBuzz')\n        elif i % 3 == 0:\n            result.append('Fizz')\n        elif i % 5 == 0:\n            result.append('Buzz')\n        else:\n            result.append(str(i))\n    return result\nExhibit 3:\ndef fizzbuzz(n: int) -&gt; list[str]:\n    fizzbuzz_list = []\n    for i in range(1, n+1):\n        fizz = ''\n        buzz = ''\n        if i%3==0 and i%5==0: fizz = 'FizzBuzz'\n        elif i%3==0: fizz = 'Fizz'\n        elif i%5==0: buzz = 'Buzz'\n        if not fizz and not buzz: fizzbuzz_list.append(str(i))\n        elif fizz and not buzz: fizzbuzz_list.append(fizz)\n        elif buzz and not fizz: fizzbuzz_list.append(buzz)\n        elif fizz and buzz: fizzbuzz_list.append(fizz + buzz)\n        elif fizz and buzz: fizzbuzz_list.append(buzz)\n        elif fizz: fizzbuzz_list.append(fizz)\n        elif buzz: fizzbuzz_list.append(buzz)\n        elif i%3==0 or i%5==0: fizzbuzz_list.append(str(i))\n        else: fizzbuzz_list.append(str(i))\n    return fizzbuzz_list\nYay, it works!"
  },
  {
    "objectID": "sloppy_fizzbuzz_blog.html#learnings",
    "href": "sloppy_fizzbuzz_blog.html#learnings",
    "title": "Sloppy Fizz Buzz: Ugly and Unpythonic LLM Code-Gen Using RL Fine-Tuning",
    "section": "Learnings",
    "text": "Learnings\nThis task was deceptively harder than I expected. I didn‚Äôt realize how hard it was to fight against the model‚Äôs tendency to generate the typical and clean version of FizzBuzz when given a generic prompt. What I have started to internalize more is that when you restrict the input to a specific prompt, you basically fix the probability distribution of the output. In a way, doing LoRA fine-tuning is like reshaping the probability distribution of the model outputs, which I guess is somewhat obvious and makes a lot of sense when you think about it.\nBut visually what I like to imagine here is that it‚Äôs almost like we‚Äôre trying to invert a bell curve. If we exaggerate the idea that ‚Äúclean‚Äù FizzBuzz is the model‚Äôs preferred output near the center and ‚Äúugly‚Äù FizzBuzz is spread out around the tail of the distribution, we‚Äôre essentially redistributing the probability mass into a very different shape. This was not as obvious to me in the beginning and would have helped me understand how much my self-imposed constraints made the task a lot harder.\nSo when we rely on the model to randomly discover ugly output, the odds of that happening are kind of gated by what kind of Python code the model has seen and whether those snippets show up during our fine-tuning process. It could discover ugliness by random chance and this signal is reinforced by our reward function. But without directly introducing new information via examples, in this restricted code-gen scenario, the LoRA adapter + RL process behaves more like a sieve that filters out code snippets from the base model that meet our ugliness criteria and from there we increase the random odds and luck of discovering ugliness via the reward function."
  },
  {
    "objectID": "sloppy_fizzbuzz_blog.html#links",
    "href": "sloppy_fizzbuzz_blog.html#links",
    "title": "Sloppy Fizz Buzz: Ugly and Unpythonic LLM Code-Gen Using RL Fine-Tuning",
    "section": "Links",
    "text": "Links\n\nGithub Repo: github.com/seantey/sloppy-fizzbuzz\nTwitter/X: @seansrr"
  }
]